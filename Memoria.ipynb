{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><p style=\"color: Turquoise;\"><strong>Proyecto Apache Spark</strong></p></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: green;\">Introducción</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](esquema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, se aborda el diseño y la implementación de una solución integral para la extracción, procesamiento, almacenamiento y visualización de datos provenientes de la Agencia Estatal de Meteorología Española (AEMET). El flujo de trabajo se estructura en cuatro etapas principales: *data sources*, *ETL*, *data warehouse* y *visualization*. Cada una de estas etapas está orquestada y gestionada mediante Apache Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: green;\">API AEMET</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde la URL indicada en el bloque de codigo siguiente, AEMET proporciona datos de las estaciones meteorológicas presentes en españa en ese momento.\n",
    "\n",
    "Paras obtener esta información, se manda una *request GET* al *endpoint* del api y ésta deberá devolver una URL con la ubicación de los datos, a la cual se manda una segunda *request GET*.\n",
    "\n",
    "Los datos vienen en formato json y se pueden cargar a pandas directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://opendata.aemet.es/opendata/sh/1ed165ab\n",
      "https://opendata.aemet.es/opendata/sh/0556af7a\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/inventarioestaciones/todasestaciones/\"\n",
    "\n",
    "querystring = {\"api_key\":\"  \"}\n",
    "\n",
    "headers = {\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "url_estaciones = requests.request(\"GET\", url, headers=headers, params=querystring).json()['datos']\n",
    "url_estaciones_metadata = requests.request(\"GET\", url, headers=headers, params=querystring).json()['metadatos']\n",
    "\n",
    "print(url_estaciones)\n",
    "print(url_estaciones_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero obtenemos los datos relativos a todas las estaciones meteorológicas de AEMET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitud</th>\n",
       "      <th>provincia</th>\n",
       "      <th>altitud</th>\n",
       "      <th>indicativo</th>\n",
       "      <th>nombre</th>\n",
       "      <th>indsinop</th>\n",
       "      <th>longitud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>394924N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>490</td>\n",
       "      <td>B013X</td>\n",
       "      <td>ESCORCA, LLUC</td>\n",
       "      <td>08304</td>\n",
       "      <td>025309E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394744N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>5</td>\n",
       "      <td>B051A</td>\n",
       "      <td>SÓLLER, PUERTO</td>\n",
       "      <td>08316</td>\n",
       "      <td>024129E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394121N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>60</td>\n",
       "      <td>B087X</td>\n",
       "      <td>BANYALBUFAR</td>\n",
       "      <td></td>\n",
       "      <td>023046E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393445N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>52</td>\n",
       "      <td>B103B</td>\n",
       "      <td>ANDRATX - SANT ELM</td>\n",
       "      <td>99103</td>\n",
       "      <td>022208E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393305N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>50</td>\n",
       "      <td>B158X</td>\n",
       "      <td>CALVIÀ, ES CAPDELLÀ</td>\n",
       "      <td></td>\n",
       "      <td>022759E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>424131N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>2467</td>\n",
       "      <td>9988B</td>\n",
       "      <td>CAP DE VAQUÈIRA</td>\n",
       "      <td>08936</td>\n",
       "      <td>005826E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>424201N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>1161</td>\n",
       "      <td>9990X</td>\n",
       "      <td>NAUT ARAN, ARTIES</td>\n",
       "      <td>08107</td>\n",
       "      <td>005237E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>424634N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>722</td>\n",
       "      <td>9994X</td>\n",
       "      <td>BOSSÒST</td>\n",
       "      <td></td>\n",
       "      <td>004123E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>430528N</td>\n",
       "      <td>NAVARRA</td>\n",
       "      <td>334</td>\n",
       "      <td>9995Y</td>\n",
       "      <td>VALCARLOS/LUZAIDE</td>\n",
       "      <td></td>\n",
       "      <td>011803W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>405517N</td>\n",
       "      <td>TERUEL</td>\n",
       "      <td>1006</td>\n",
       "      <td>9998X</td>\n",
       "      <td>BELLO</td>\n",
       "      <td></td>\n",
       "      <td>012940W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>947 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     latitud      provincia altitud indicativo               nombre indsinop  \\\n",
       "0    394924N  ILLES BALEARS     490      B013X        ESCORCA, LLUC    08304   \n",
       "1    394744N  ILLES BALEARS       5      B051A       SÓLLER, PUERTO    08316   \n",
       "2    394121N  ILLES BALEARS      60      B087X          BANYALBUFAR            \n",
       "3    393445N  ILLES BALEARS      52      B103B   ANDRATX - SANT ELM    99103   \n",
       "4    393305N  ILLES BALEARS      50      B158X  CALVIÀ, ES CAPDELLÀ            \n",
       "..       ...            ...     ...        ...                  ...      ...   \n",
       "942  424131N         LLEIDA    2467      9988B      CAP DE VAQUÈIRA    08936   \n",
       "943  424201N         LLEIDA    1161      9990X   NAUT ARAN, ARTIES     08107   \n",
       "944  424634N         LLEIDA     722      9994X              BOSSÒST            \n",
       "945  430528N        NAVARRA     334      9995Y    VALCARLOS/LUZAIDE            \n",
       "946  405517N         TERUEL    1006      9998X                BELLO            \n",
       "\n",
       "    longitud  \n",
       "0    025309E  \n",
       "1    024129E  \n",
       "2    023046E  \n",
       "3    022208E  \n",
       "4    022759E  \n",
       "..       ...  \n",
       "942  005826E  \n",
       "943  005237E  \n",
       "944  004123E  \n",
       "945  011803W  \n",
       "946  012940W  \n",
       "\n",
       "[947 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = requests.request(\"GET\", url_estaciones, headers=headers, params=querystring).json()\n",
    "df_estaciones = pd.DataFrame(data)\n",
    "df_estaciones.to_csv('datos.csv', index=False, header=True)\n",
    "df_estaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que contiene la columna provincias y las asociaremos a su correspondiente comunidad autónoma para posteriormente obtener esta nueva categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ILLES BALEARS', 'BALEARES', 'LAS PALMAS', 'STA. CRUZ DE TENERIFE',\n",
       "       'SANTA CRUZ DE TENERIFE', 'TARRAGONA', 'BARCELONA', 'GIRONA',\n",
       "       'NAVARRA', 'GIPUZKOA', 'ARABA/ALAVA', 'BIZKAIA', 'CANTABRIA',\n",
       "       'ASTURIAS', 'LEON', 'LUGO', 'A CORUÑA', 'PONTEVEDRA', 'OURENSE',\n",
       "       'SORIA', 'BURGOS', 'SEGOVIA', 'VALLADOLID', 'PALENCIA', 'AVILA',\n",
       "       'MADRID', 'SALAMANCA', 'ZAMORA', 'GUADALAJARA', 'CUENCA', 'TOLEDO',\n",
       "       'CACERES', 'ALBACETE', 'CIUDAD REAL', 'BADAJOZ', 'CORDOBA',\n",
       "       'HUELVA', 'CEUTA', 'JAEN', 'GRANADA', 'ALMERIA', 'SEVILLA',\n",
       "       'CADIZ', 'MELILLA', 'MALAGA', 'MURCIA', 'ALICANTE', 'VALENCIA',\n",
       "       'TERUEL', 'CASTELLON', 'LA RIOJA', 'HUESCA', 'ZARAGOZA', 'LLEIDA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_estaciones.provincia.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitud</th>\n",
       "      <th>provincia</th>\n",
       "      <th>altitud</th>\n",
       "      <th>indicativo</th>\n",
       "      <th>nombre</th>\n",
       "      <th>indsinop</th>\n",
       "      <th>longitud</th>\n",
       "      <th>comunidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>394924N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>490</td>\n",
       "      <td>B013X</td>\n",
       "      <td>ESCORCA, LLUC</td>\n",
       "      <td>08304</td>\n",
       "      <td>025309E</td>\n",
       "      <td>Islas Baleares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394744N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>5</td>\n",
       "      <td>B051A</td>\n",
       "      <td>SÓLLER, PUERTO</td>\n",
       "      <td>08316</td>\n",
       "      <td>024129E</td>\n",
       "      <td>Islas Baleares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394121N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>60</td>\n",
       "      <td>B087X</td>\n",
       "      <td>BANYALBUFAR</td>\n",
       "      <td></td>\n",
       "      <td>023046E</td>\n",
       "      <td>Islas Baleares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393445N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>52</td>\n",
       "      <td>B103B</td>\n",
       "      <td>ANDRATX - SANT ELM</td>\n",
       "      <td>99103</td>\n",
       "      <td>022208E</td>\n",
       "      <td>Islas Baleares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393305N</td>\n",
       "      <td>ILLES BALEARS</td>\n",
       "      <td>50</td>\n",
       "      <td>B158X</td>\n",
       "      <td>CALVIÀ, ES CAPDELLÀ</td>\n",
       "      <td></td>\n",
       "      <td>022759E</td>\n",
       "      <td>Islas Baleares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>424131N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>2467</td>\n",
       "      <td>9988B</td>\n",
       "      <td>CAP DE VAQUÈIRA</td>\n",
       "      <td>08936</td>\n",
       "      <td>005826E</td>\n",
       "      <td>Cataluña</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>424201N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>1161</td>\n",
       "      <td>9990X</td>\n",
       "      <td>NAUT ARAN, ARTIES</td>\n",
       "      <td>08107</td>\n",
       "      <td>005237E</td>\n",
       "      <td>Cataluña</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>424634N</td>\n",
       "      <td>LLEIDA</td>\n",
       "      <td>722</td>\n",
       "      <td>9994X</td>\n",
       "      <td>BOSSÒST</td>\n",
       "      <td></td>\n",
       "      <td>004123E</td>\n",
       "      <td>Cataluña</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>430528N</td>\n",
       "      <td>NAVARRA</td>\n",
       "      <td>334</td>\n",
       "      <td>9995Y</td>\n",
       "      <td>VALCARLOS/LUZAIDE</td>\n",
       "      <td></td>\n",
       "      <td>011803W</td>\n",
       "      <td>Navarra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>405517N</td>\n",
       "      <td>TERUEL</td>\n",
       "      <td>1006</td>\n",
       "      <td>9998X</td>\n",
       "      <td>BELLO</td>\n",
       "      <td></td>\n",
       "      <td>012940W</td>\n",
       "      <td>Aragón</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>947 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     latitud      provincia altitud indicativo               nombre indsinop  \\\n",
       "0    394924N  ILLES BALEARS     490      B013X        ESCORCA, LLUC    08304   \n",
       "1    394744N  ILLES BALEARS       5      B051A       SÓLLER, PUERTO    08316   \n",
       "2    394121N  ILLES BALEARS      60      B087X          BANYALBUFAR            \n",
       "3    393445N  ILLES BALEARS      52      B103B   ANDRATX - SANT ELM    99103   \n",
       "4    393305N  ILLES BALEARS      50      B158X  CALVIÀ, ES CAPDELLÀ            \n",
       "..       ...            ...     ...        ...                  ...      ...   \n",
       "942  424131N         LLEIDA    2467      9988B      CAP DE VAQUÈIRA    08936   \n",
       "943  424201N         LLEIDA    1161      9990X   NAUT ARAN, ARTIES     08107   \n",
       "944  424634N         LLEIDA     722      9994X              BOSSÒST            \n",
       "945  430528N        NAVARRA     334      9995Y    VALCARLOS/LUZAIDE            \n",
       "946  405517N         TERUEL    1006      9998X                BELLO            \n",
       "\n",
       "    longitud       comunidad  \n",
       "0    025309E  Islas Baleares  \n",
       "1    024129E  Islas Baleares  \n",
       "2    023046E  Islas Baleares  \n",
       "3    022208E  Islas Baleares  \n",
       "4    022759E  Islas Baleares  \n",
       "..       ...             ...  \n",
       "942  005826E        Cataluña  \n",
       "943  005237E        Cataluña  \n",
       "944  004123E        Cataluña  \n",
       "945  011803W         Navarra  \n",
       "946  012940W          Aragón  \n",
       "\n",
       "[947 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provincia_comunidad = {\n",
    "\n",
    "    'ARABA/ALAVA': 'País Vasco',\n",
    "    'Albacete': 'Castilla-La Mancha',\n",
    "    'Alicante': 'Comunidad Valenciana',\n",
    "    'ALMERIA': 'Andalucía',\n",
    "    'Asturias': 'Asturias',\n",
    "    'AVILA': 'Castilla y León',\n",
    "    'Badajoz': 'Extremadura',\n",
    "    'Barcelona': 'Cataluña',\n",
    "    'Burgos': 'Castilla y León',\n",
    "    'CACERES': 'Extremadura',\n",
    "    'CADIZ': 'Andalucía',\n",
    "    'Cantabria': 'Cantabria',\n",
    "    'CASTELLON': 'Comunidad Valenciana',\n",
    "    'Ciudad Real': 'Castilla-La Mancha',\n",
    "    'CORDOBA': 'Andalucía',\n",
    "    'Cuenca': 'Castilla-La Mancha',\n",
    "    'Girona': 'Cataluña',\n",
    "    'Granada': 'Andalucía',\n",
    "    'Guadalajara': 'Castilla-La Mancha',\n",
    "    'Gipuzkoa': 'País Vasco',\n",
    "    'Huelva': 'Andalucía',\n",
    "    'Huesca': 'Aragón',\n",
    "    'ILLES BALEARS': 'Islas Baleares',\n",
    "    'JAEN': 'Andalucía',\n",
    "    'A CORUÑA': 'Galicia',\n",
    "    'La Rioja': 'La Rioja',\n",
    "    'Las Palmas': 'Canarias',\n",
    "    'LEON': 'Castilla y León',\n",
    "    'LLEIDA': 'Cataluña',\n",
    "    'Lugo': 'Galicia',\n",
    "    'Madrid': 'Comunidad de Madrid',\n",
    "    'MALAGA': 'Andalucía',\n",
    "    'Murcia': 'Región de Murcia',\n",
    "    'Navarra': 'Navarra',\n",
    "    'OURENSE': 'Galicia',\n",
    "    'Palencia': 'Castilla y León',\n",
    "    'Pontevedra': 'Galicia',\n",
    "    'Salamanca': 'Castilla y León',\n",
    "    'SANTA CRUZ DE TENERIFE': 'Canarias',\n",
    "    'Segovia': 'Castilla y León',\n",
    "    'Sevilla': 'Andalucía',\n",
    "    'Soria': 'Castilla y León',\n",
    "    'Tarragona': 'Cataluña',\n",
    "    'Teruel': 'Aragón',\n",
    "    'Toledo': 'Castilla-La Mancha',\n",
    "    'Valencia': 'Comunidad Valenciana',\n",
    "    'Valladolid': 'Castilla y León',\n",
    "    'BIZKAIA': 'País Vasco',\n",
    "    'Zamora': 'Castilla y León',\n",
    "    'Zaragoza': 'Aragón',\n",
    "    'Ceuta': 'Ceuta',\n",
    "    'Melilla': 'Melilla',\n",
    "    'STA. CRUZ DE TENERIFE': 'Canarias',\n",
    "    'Baleares': 'Islas Baleares',\n",
    "}\n",
    "provincia_comunidad = {k.upper(): v for k, v in provincia_comunidad.items()}\n",
    "df_estaciones['comunidad'] = df_estaciones['provincia'].map(provincia_comunidad)\n",
    "df_estaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde la URL indicada en el bloque de codigo siguiente, AEMET proporciona datos meteorológicos diarios tomados por todas las estaciones meteorológicas presentes en españa en ese momento, comenzando en 1991 y hasta hoy. \n",
    "\n",
    "Para obtenerlos, se manda una request GET al endpoint del api y ésta deberá devolver una URL con la ubicación de los datos, a la cual se manda una segunda request GET.\n",
    "\n",
    "Los datos vienen en formato json y se pueden cargar a pandas directamente.\n",
    "\n",
    "Existen varias limitaciones: sólo está permitido obtener datos de un rango de 32 días consecutivos o menos (si se quiere un período mas largo, hay que lanzar mas GETs) y sólo se pueden mandar 50 requests por minuto por API key.\n",
    "\n",
    "El siguiente código tiene en cuenta esas limitaciones y obtiene todos los datos entre *start_date* y *end_date*, dividiendo si fuera necesario el rango de fechas en lotes de tamaño 32 * (50/2) (ya que hay que mandar 2 request para obtener un conjunto de datos) y poniendo el tiempo de espera suficiente (indicado por *timeout_between_requests*). También va dejando logs por pantalla del avance del proceso, y si se perdieran datos en el camino, informa de cuales se han perdido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo datos para las fechas comprendidas entre 01-ene. 1991 y 06-feb. 1993...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 8.627 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 07-feb. 1993 y 16-mar. 1995...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 15.097 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 17-mar. 1995 y 22-abr. 1997...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 16.423 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 23-abr. 1997 y 30-may. 1999...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 11.231 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 31-may. 1999 y 06-jul. 2001...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 12.017 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 07-jul. 2001 y 13-ago. 2003...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 11.905 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 14-ago. 2003 y 19-sep. 2005...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 26.229 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 20-sep. 2005 y 27-oct. 2007...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 19.185 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 28-oct. 2007 y 03-dic. 2009...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 33.838 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 04-dic. 2009 y 10-ene. 2012...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 45.07 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 11-ene. 2012 y 16-feb. 2014...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 48.442 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 17-feb. 2014 y 25-mar. 2016...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 27.762 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 26-mar. 2016 y 02-may. 2018...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 33.197 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 03-may. 2018 y 08-jun. 2020...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 46.229 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 09-jun. 2020 y 16-jul. 2022...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 37.795 s.\n",
      "Obteniendo datos para las fechas comprendidas entre 17-jul. 2022 y 01-ene. 2024...\n",
      "¡Datos obtenidos!. Tiempo transcurrido: 36.786 s.\n",
      "¡Tarea completada!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "api_key = \"\"\n",
    "def fetch_df_from_period(start_date, end_date, api_key):\n",
    "    fechaIniStr = start_date.strftime(\"%Y-%m-%dT%H:%M:%SUTC\")\n",
    "    fechaFinStr = end_date.strftime(\"%Y-%m-%dT%H:%M:%SUTC\")\n",
    "    url = f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{fechaIniStr}/fechafin/{fechaFinStr}/todasestaciones\"\n",
    "    querystring = {\"api_key\": api_key}\n",
    "    headers = {'cache-control': \"no-cache\"}\n",
    "\n",
    "    success = False\n",
    "    while not success:  # Intenta hasta que tenga éxito\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "            response.raise_for_status()\n",
    "            if response:\n",
    "                url2 = response.json()['datos']\n",
    "                data = requests.get(url2, headers=headers, params=querystring)\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data.json())\n",
    "                    success = True\n",
    "                    return df\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(\"Error de conexión:\", e)\n",
    "            print(f'No se obtuvieron datos para el período entre {start_date.strftime(\"%d-%b %Y\")} y {end_date.strftime(\"%d-%b %Y\")}')\n",
    "            time.sleep(5)  # Espera un poco antes de intentarlo de nuevo\n",
    "\n",
    "    if not success:\n",
    "        print(f'No se pudieron obtener datos después de múltiples intentos para el período entre {start_date.strftime(\"%d-%b %Y\")} y {end_date.strftime(\"%d-%b %Y\")}')\n",
    "        return pd.DataFrame()  # Si no puede obtener los datos, devuelve un DataFrame vacío\n",
    "\n",
    "start_date = datetime(1991, 1, 1) \n",
    "end_date = datetime(2023, 12, 31)\n",
    "# el intervalo máximo de días que deja pedir en una llamada al API de AEMET es 32 (incluyendo el primer día y el último).\n",
    "interval = 32 \n",
    "# el numero maximo de llamadas por minuto permitidas para un mismo API key de AEMET es 50.\n",
    "batch_size = int(48 / 2) # Dividimos entre 2 porque por cada intervalo de fecha son 2 requests para obtener los datos.\n",
    "timeout_between_requests = 60 # (un minuto)\n",
    "batch_start_date = start_date - timedelta(days = 1)\n",
    "\n",
    "all_batch_dfs = []\n",
    "while batch_start_date <= end_date:\n",
    "        task_start_time = time.time()\n",
    "        batch_end_date = min(batch_start_date + (batch_size) * timedelta(days=interval), end_date)\n",
    "        formatted_start_date = (batch_start_date + timedelta(days = 1)).strftime(\"%d-%b %Y\")\n",
    "        formatted_end_date = batch_end_date.strftime(\"%d-%b %Y\")\n",
    "        print(f'Obteniendo datos para las fechas comprendidas entre {formatted_start_date} y {formatted_end_date}...')\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = [\n",
    "                        executor.submit(\n",
    "                                fetch_df_from_period,\n",
    "                                batch_start_date + timedelta(days=interval * (i-1)+1),\n",
    "                                min(batch_start_date + timedelta(days=interval * i), end_date),\n",
    "                                api_key\n",
    "                        )\n",
    "                        for i in range(1, min(batch_size, int((batch_end_date - batch_start_date).days / interval)+1)+1)\n",
    "                ]\n",
    "        batch_df = pd.concat([future.result() for future in concurrent.futures.as_completed(futures)], ignore_index=True)\n",
    "        all_batch_dfs.append(batch_df)\n",
    "        task_end_time = time.time()\n",
    "        elapsed_time = task_end_time - task_start_time\n",
    "        print(f'¡Datos obtenidos!. Tiempo transcurrido: {round(elapsed_time, 3)} s.')\n",
    "        batch_start_date = min(batch_end_date, end_date)\n",
    "        if batch_start_date >= end_date:\n",
    "                break\n",
    "        remaining_time = max(0, timeout_between_requests - (task_end_time - task_start_time))\n",
    "        if remaining_time == 0:\n",
    "                # Si la llamada al API duró mas de 60 segundos, metemos un timeout de 60 segundos para el siguiente lote\n",
    "                remaining_time = 60\n",
    "        while remaining_time > 0:\n",
    "                print(f'Esperando {int(remaining_time)} segundos más antes del siguiente lote...', end='\\r')\n",
    "                time.sleep(1)\n",
    "                remaining_time -= 1\n",
    "        print(' '*50, end='\\r')\n",
    "        time.sleep(5)  \n",
    "print('¡Tarea completada!')\n",
    "final_df = pd.concat(all_batch_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergeamos las dos matrices y ordenamos los datos por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>indicativo</th>\n",
       "      <th>nombre_x</th>\n",
       "      <th>provincia_x</th>\n",
       "      <th>altitud_x</th>\n",
       "      <th>tmed</th>\n",
       "      <th>prec</th>\n",
       "      <th>tmin</th>\n",
       "      <th>horatmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>...</th>\n",
       "      <th>presMin</th>\n",
       "      <th>horaPresMin</th>\n",
       "      <th>sol</th>\n",
       "      <th>latitud</th>\n",
       "      <th>provincia_y</th>\n",
       "      <th>altitud_y</th>\n",
       "      <th>nombre_y</th>\n",
       "      <th>indsinop</th>\n",
       "      <th>longitud</th>\n",
       "      <th>comunidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81153</th>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>9573X</td>\n",
       "      <td>ALCAÑIZ</td>\n",
       "      <td>TERUEL</td>\n",
       "      <td>334</td>\n",
       "      <td>7,8</td>\n",
       "      <td>0,0</td>\n",
       "      <td>2,5</td>\n",
       "      <td>21:35</td>\n",
       "      <td>13,0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>410329N</td>\n",
       "      <td>TERUEL</td>\n",
       "      <td>334</td>\n",
       "      <td>ALCAÑIZ</td>\n",
       "      <td>08164</td>\n",
       "      <td>000830W</td>\n",
       "      <td>Aragón</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421018</th>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>1249I</td>\n",
       "      <td>OVIEDO</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>336</td>\n",
       "      <td>7,3</td>\n",
       "      <td>0,0</td>\n",
       "      <td>3,0</td>\n",
       "      <td>08:30</td>\n",
       "      <td>11,6</td>\n",
       "      <td>...</td>\n",
       "      <td>983,4</td>\n",
       "      <td>24</td>\n",
       "      <td>7,4</td>\n",
       "      <td>432112N</td>\n",
       "      <td>ASTURIAS</td>\n",
       "      <td>336</td>\n",
       "      <td>OVIEDO</td>\n",
       "      <td>08015</td>\n",
       "      <td>055227W</td>\n",
       "      <td>Asturias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843274</th>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>1466A</td>\n",
       "      <td>SILLEDA</td>\n",
       "      <td>PONTEVEDRA</td>\n",
       "      <td>435</td>\n",
       "      <td>4,8</td>\n",
       "      <td>0,0</td>\n",
       "      <td>0,0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>424203N</td>\n",
       "      <td>PONTEVEDRA</td>\n",
       "      <td>435</td>\n",
       "      <td>SILLEDA</td>\n",
       "      <td></td>\n",
       "      <td>081528W</td>\n",
       "      <td>Galicia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059795</th>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>2150H</td>\n",
       "      <td>LA PINILLA, ESTACIÓN DE ESQUÍ</td>\n",
       "      <td>SEGOVIA</td>\n",
       "      <td>1798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>411131N</td>\n",
       "      <td>SEGOVIA</td>\n",
       "      <td>1798</td>\n",
       "      <td>LA PINILLA, ESTACIÓN DE ESQUÍ</td>\n",
       "      <td>08143</td>\n",
       "      <td>032831W</td>\n",
       "      <td>Castilla y León</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391220</th>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>1014</td>\n",
       "      <td>HONDARRIBIA, MALKARROA</td>\n",
       "      <td>GIPUZKOA</td>\n",
       "      <td>4</td>\n",
       "      <td>10,1</td>\n",
       "      <td>0,0</td>\n",
       "      <td>6,0</td>\n",
       "      <td>23:59</td>\n",
       "      <td>14,2</td>\n",
       "      <td>...</td>\n",
       "      <td>1024,2</td>\n",
       "      <td>24</td>\n",
       "      <td>5,8</td>\n",
       "      <td>432125N</td>\n",
       "      <td>GIPUZKOA</td>\n",
       "      <td>4</td>\n",
       "      <td>HONDARRIBIA, MALKARROA</td>\n",
       "      <td>08029</td>\n",
       "      <td>014732W</td>\n",
       "      <td>País Vasco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             fecha indicativo                       nombre_x provincia_x  \\\n",
       "81153   1991-01-01      9573X                        ALCAÑIZ      TERUEL   \n",
       "1421018 1991-01-01      1249I                         OVIEDO    ASTURIAS   \n",
       "1843274 1991-01-01      1466A                        SILLEDA  PONTEVEDRA   \n",
       "1059795 1991-01-01      2150H  LA PINILLA, ESTACIÓN DE ESQUÍ     SEGOVIA   \n",
       "391220  1991-01-01       1014         HONDARRIBIA, MALKARROA    GIPUZKOA   \n",
       "\n",
       "        altitud_x  tmed prec tmin horatmin  tmax  ... presMin horaPresMin  \\\n",
       "81153         334   7,8  0,0  2,5    21:35  13,0  ...     NaN         NaN   \n",
       "1421018       336   7,3  0,0  3,0    08:30  11,6  ...   983,4          24   \n",
       "1843274       435   4,8  0,0  0,0      NaN   9,5  ...     NaN         NaN   \n",
       "1059795      1798   NaN  0,0  NaN      NaN   NaN  ...     NaN         NaN   \n",
       "391220          4  10,1  0,0  6,0    23:59  14,2  ...  1024,2          24   \n",
       "\n",
       "         sol  latitud provincia_y altitud_y                       nombre_y  \\\n",
       "81153    NaN  410329N      TERUEL       334                        ALCAÑIZ   \n",
       "1421018  7,4  432112N    ASTURIAS       336                         OVIEDO   \n",
       "1843274  NaN  424203N  PONTEVEDRA       435                        SILLEDA   \n",
       "1059795  NaN  411131N     SEGOVIA      1798  LA PINILLA, ESTACIÓN DE ESQUÍ   \n",
       "391220   5,8  432125N    GIPUZKOA         4         HONDARRIBIA, MALKARROA   \n",
       "\n",
       "        indsinop longitud        comunidad  \n",
       "81153      08164  000830W           Aragón  \n",
       "1421018    08015  055227W         Asturias  \n",
       "1843274           081528W          Galicia  \n",
       "1059795    08143  032831W  Castilla y León  \n",
       "391220     08029  014732W       País Vasco  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = final_df.merge(df_estaciones, on='indicativo', how='inner')\n",
    "df_final = df_final.sort_values('fecha')\n",
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el dataframe que contiene los datos en un archivo .csv para su posterior estudio con Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('dataset_1991_to_2023.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las columnas y filas no resultan de interés; pero no se van a eliminar todavía pues la parte de preprocesamiento de datos, se va a realizar con Apache Spark en lugar de usar Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: green;\"> Spark ETL </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si encuentra Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos una sesión Spark que podemos monitorizar desde http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Aigarciab:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[10]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AEMET_Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e91f8e2d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AEMET_Spark\")\n",
    "    .master(\"local[10]\")\n",
    "    .config(\"spark.driver.memory\", \"7g\")  # Configura la memoria del driver a 7 gigabytes\n",
    "    .config(\"spark.driver.cores\", \"3\")  # Configura el número de núcleos utilizados a 5\n",
    "    .config(\"spark.executor.instances\", \"2\")  # Configura el número de ejecutores a 5\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos CSV en un dataframe para su posterior transformación. ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+-----------+---------+----+----+----+--------+----+--------+-------+----+--------+-----+---------+-----+---------+-----+---------+-------+-----------+-------+-----------+----+-------+-----------+---------+--------------------+--------+--------+---------------+\n",
      "|     fecha|indicativo|            nombre_x|provincia_x|altitud_x|tmed|prec|tmin|horatmin|tmax|horatmax|hrMedia| dir|velmedia|racha|horaracha|hrMax|horaHrMax|hrMin|horaHrMin|presMax|horaPresMax|presMin|horaPresMin| sol|latitud|provincia_y|altitud_y|            nombre_y|indsinop|longitud|      comunidad|\n",
      "+----------+----------+--------------------+-----------+---------+----+----+----+--------+----+--------+-------+----+--------+-----+---------+-----+---------+-----+---------+-------+-----------+-------+-----------+----+-------+-----------+---------+--------------------+--------+--------+---------------+\n",
      "|1991-01-01|     9573X|             ALCAÑIZ|     TERUEL|      334| 7,8| 0,0| 2,5|   21:35|13,0|   14:30|   NULL|  27|     1,7| 11,4|    10:40| NULL|     NULL| NULL|     NULL|   NULL|       NULL|   NULL|       NULL|NULL|410329N|     TERUEL|      334|             ALCAÑIZ|    8164| 000830W|         Aragón|\n",
      "|1991-01-01|     1249I|              OVIEDO|   ASTURIAS|      336| 7,3| 0,0| 3,0|   08:30|11,6|   14:50|     71|  16|     0,6|  4,4|    04:45| NULL|     NULL| NULL|     NULL|  988,1|         09|  983,4|         24| 7,4|432112N|   ASTURIAS|      336|              OVIEDO|    8015| 055227W|       Asturias|\n",
      "|1991-01-01|     1466A|             SILLEDA| PONTEVEDRA|      435| 4,8| 0,0| 0,0|    NULL| 9,5|    NULL|   NULL|NULL|    NULL| NULL|     NULL| NULL|     NULL| NULL|     NULL|   NULL|       NULL|   NULL|       NULL|NULL|424203N| PONTEVEDRA|      435|             SILLEDA|    NULL| 081528W|        Galicia|\n",
      "|1991-01-01|     2150H|LA PINILLA, ESTAC...|    SEGOVIA|     1798|NULL| 0,0|NULL|    NULL|NULL|    NULL|   NULL|NULL|    NULL| NULL|     NULL| NULL|     NULL| NULL|     NULL|   NULL|       NULL|   NULL|       NULL|NULL|411131N|    SEGOVIA|     1798|LA PINILLA, ESTAC...|    8143| 032831W|Castilla y León|\n",
      "|1991-01-01|      1014|HONDARRIBIA, MALK...|   GIPUZKOA|        4|10,1| 0,0| 6,0|   23:59|14,2|   13:00|     79|  30|     1,9|  6,1|    02:35| NULL|     NULL| NULL|     NULL| 1028,6|         10| 1024,2|         24| 5,8|432125N|   GIPUZKOA|        4|HONDARRIBIA, MALK...|    8029| 014732W|     País Vasco|\n",
      "+----------+----------+--------------------+-----------+---------+----+----+----+--------+----+--------+-------+----+--------+-----+---------+-----+---------+-----+---------+-------+-----------+-------+-----------+----+-------+-----------+---------+--------------------+--------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Dimension de la matriz:  6111264 x 32\n",
      "[('fecha', 'date'), ('indicativo', 'string'), ('nombre_x', 'string'), ('provincia_x', 'string'), ('altitud_x', 'int'), ('tmed', 'string'), ('prec', 'string'), ('tmin', 'string'), ('horatmin', 'string'), ('tmax', 'string'), ('horatmax', 'string'), ('hrMedia', 'int'), ('dir', 'int'), ('velmedia', 'string'), ('racha', 'string'), ('horaracha', 'string'), ('hrMax', 'int'), ('horaHrMax', 'string'), ('hrMin', 'int'), ('horaHrMin', 'string'), ('presMax', 'string'), ('horaPresMax', 'string'), ('presMin', 'string'), ('horaPresMin', 'string'), ('sol', 'string'), ('latitud', 'string'), ('provincia_y', 'string'), ('altitud_y', 'int'), ('nombre_y', 'string'), ('indsinop', 'int'), ('longitud', 'string'), ('comunidad', 'string')]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"dataset_1991_to_2023.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "print('Dimension de la matriz: ', df.count(), 'x', len(df.columns))\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Información de las variables iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fecha --> Fecha del dia (AAAA-MM-DD)\n",
    "- latitud --> Latitud geográfica de la estación\n",
    "- provincia --> Provincia donde reside la estación\n",
    "- comunidad --> Comunidad Autónoma donde reside la estación\n",
    "- indicativo --> Indicativo climatológico de la estación\n",
    "- altitud --> Altitud de la estación, \"unidad\": \"m\"\n",
    "- nombre --> Ubicación de la estación\n",
    "- indsinop --> Indicativo sinóptico\n",
    "- longitud --> Longitud geográfica de la estación\n",
    "- tmed --> Temperatura media diaria, \"unidad\": \"°C\"\n",
    "- prec --> Precipitación diaria de 07 a 07, \"unidad\": \"mm (Ip = inferior a 0,1 mm) (Acum = Precipitación acumulada)\"\n",
    "- tmin --> Temperatura Mínima del día, \"unidad\": \"°C\"\n",
    "- horatmin --> Hora y minuto de la temperatura mínima, \"unidad\": \"UTC\"\n",
    "- tmax --> Temperatura Máxima del día, \"unidad\": \"°C\"\n",
    "- horatmax --> Hora y minuto de la temperatura máxima\n",
    "- dir --> Dirección de la racha máxima, \"unidad\": \"decenas de grado (99 = dirección variable)(88 = sin dato)\"\n",
    "- velmedia --> Velocidad media del viento, \"unidad\": \"m/s\"\n",
    "- racha --> Racha máxima del viento, \"unidad\": \"m/s\"\n",
    "- horaracha --> Hora y minuto de la racha máxima, \"unidad\": \"UTC\"\n",
    "- sol --> Insolación, \"unidad\": \"horas\"\n",
    "- presmax --> Presión máxima al nivel de referencia de la estación, \"unidad\": \"hPa\"\n",
    "- horapresmax --> Hora de la presión máxima (redondeada a la hora entera más próxima), \"unidad\": \"hPa\"\n",
    "- presmin --> Presión mínima al nivel de referencia de la estación, \"unidad\": \"hPa\"\n",
    "- horapresmin --> Hora de la presión mínima (redondeada a la hora entera más próxima), \"unidad\": \"hPa\"\n",
    "- hrmedia --> Humedad relativa media diaria, \"unidad\": \"%\"\n",
    "- hrmax --> Humedad relativa máxima diaria, \"unidad\": \"%\"\n",
    "- horahrmax --> Hora de la humedad relativa máxima diaria, \"unidad\": \"UTC\"\n",
    "- hrmin --> Humedad relativa mínima diaria, \"unidad\": \"hPa\"\n",
    "- horahrmin --> Hora de la humedad relativa mínima diaria, \"unidad\": \"UTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, avg, col, translate\n",
    "\n",
    "def ETL1(df):\n",
    "    # Eliminamos las columnas y filas que no resultan de interes o nos proporcionan información repetida\n",
    "    df = df.filter(df[\"fecha\"] != '2024-01-01')\n",
    "    df = df.drop('nombre_y','provincia_y','altitud_y','hrMedia','horaPresMax','horaPresMin','hrMax','hrMin','horaHRMin','horaHRMax','horaracha','indsinop','dir')\n",
    "    df = df.withColumnRenamed('nombre_x','nombre').withColumnRenamed('provincia_x','provincia').withColumnRenamed('altitud_x','altitud')\n",
    "    df = df.dropna(subset=['tmed'])\n",
    "\n",
    "    # Se definen 3 nuevas columnas\n",
    "    df = df.withColumn(\"año\", split(df[\"fecha\"], \"-\")[0])\n",
    "    df = df.withColumn(\"mes\", split(df[\"fecha\"], \"-\")[1])\n",
    "    df = df.withColumn(\"dia\", split(df[\"fecha\"], \"-\")[2])\n",
    "\n",
    "    # Cambiamos el tipo de datos de algunas columnas a float\n",
    "    for column in [\"tmed\", \"prec\", \"tmin\", \"tmax\", \"velmedia\", \"racha\", \"presMax\", \"presMin\"]:\n",
    "        df = df.withColumn(column, translate(col(column), \",\", \".\"))\n",
    "        df = df.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "    # Ordenamod el dataframe por fecha\n",
    "    df = df.orderBy('fecha')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL2(df):\n",
    "    # Se eliminan más columnas que no resultan de interés\n",
    "    df = df.drop('presMax', 'presMin', 'sol', 'velmedia', 'indicativo', 'horatmin','horatmax')\n",
    "    ordered_columns = ['fecha', 'comunidad', 'provincia', 'nombre', 'altitud', 'latitud', 'longitud', 'tmed', 'prec', 'tmin', 'tmax', 'racha']\n",
    "    df = df.select(*ordered_columns)\n",
    "\n",
    "    # Calculamos la temperatura y precipitación promedio de cada mes y año a nivel nacional, sin distinción de estaciones\n",
    "#     mes_año = [\"tmed_mes_año\", \"prec_mes_año\", \"tmin_mes_año\", \"tmax_mes_año\"]\n",
    "#     tp = [\"tmed\", \"prec\", \"tmin\", \"tmax\"]\n",
    "#     for i in range (0,4):\n",
    "#             df_aux = df.groupBy(\"año\", \"mes\").agg(avg(tp[i]).alias(mes_año[i])).withColumn(mes_año[i], col(mes_año[i]).cast(\"float\"))\n",
    "#             df = df.join(df_aux, [\"año\", \"mes\"], \"inner\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------+-------+----+----+----+--------+----+--------+--------+-----+-------+-------+----+-------+--------+---------------+----+---+---+\n",
      "|     fecha|indicativo|              nombre| provincia|altitud|tmed|prec|tmin|horatmin|tmax|horatmax|velmedia|racha|presMax|presMin| sol|latitud|longitud|      comunidad| año|mes|dia|\n",
      "+----------+----------+--------------------+----------+-------+----+----+----+--------+----+--------+--------+-----+-------+-------+----+-------+--------+---------------+----+---+---+\n",
      "|1991-01-01|     9573X|             ALCAÑIZ|    TERUEL|    334| 7.8| 0.0| 2.5|   21:35|13.0|   14:30|     1.7| 11.4|   NULL|   NULL|NULL|410329N| 000830W|         Aragón|1991| 01| 01|\n",
      "|1991-01-01|     1249I|              OVIEDO|  ASTURIAS|    336| 7.3| 0.0| 3.0|   08:30|11.6|   14:50|     0.6|  4.4|  988.1|  983.4| 7,4|432112N| 055227W|       Asturias|1991| 01| 01|\n",
      "|1991-01-01|     1466A|             SILLEDA|PONTEVEDRA|    435| 4.8| 0.0| 0.0|    NULL| 9.5|    NULL|    NULL| NULL|   NULL|   NULL|NULL|424203N| 081528W|        Galicia|1991| 01| 01|\n",
      "|1991-01-01|      1014|HONDARRIBIA, MALK...|  GIPUZKOA|      4|10.1| 0.0| 6.0|   23:59|14.2|   13:00|     1.9|  6.1| 1028.6| 1024.2| 5,8|432125N| 014732W|     País Vasco|1991| 01| 01|\n",
      "|1991-01-01|      2331|   BURGOS AEROPUERTO|    BURGOS|    891| 1.6| 0.0|-1.4|   23:59| 4.6|   10:50|     3.1|  6.7|  925.6|  922.7| 2,7|422125N| 033717W|Castilla y León|1991| 01| 01|\n",
      "+----------+----------+--------------------+----------+-------+----+----+----+--------+----+--------+--------+-----+-------+-------+----+-------+--------+---------------+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_after_ETL1 = ETL1(df)\n",
    "df_after_ETL1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+--------------------+-------+-------+--------+----+----+----+----+-----+\n",
      "|     fecha|      comunidad| provincia|              nombre|altitud|latitud|longitud|tmed|prec|tmin|tmax|racha|\n",
      "+----------+---------------+----------+--------------------+-------+-------+--------+----+----+----+----+-----+\n",
      "|1991-01-01|         Aragón|    TERUEL|             ALCAÑIZ|    334|410329N| 000830W| 7.8| 0.0| 2.5|13.0| 11.4|\n",
      "|1991-01-01|       Asturias|  ASTURIAS|              OVIEDO|    336|432112N| 055227W| 7.3| 0.0| 3.0|11.6|  4.4|\n",
      "|1991-01-01|        Galicia|PONTEVEDRA|             SILLEDA|    435|424203N| 081528W| 4.8| 0.0| 0.0| 9.5| NULL|\n",
      "|1991-01-01|     País Vasco|  GIPUZKOA|HONDARRIBIA, MALK...|      4|432125N| 014732W|10.1| 0.0| 6.0|14.2|  6.1|\n",
      "|1991-01-01|Castilla y León|    BURGOS|   BURGOS AEROPUERTO|    891|422125N| 033717W| 1.6| 0.0|-1.4| 4.6|  6.7|\n",
      "+----------+---------------+----------+--------------------+-------+-------+--------+----+----+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Dimension de la matriz:  5892675 x 12\n",
      "[('fecha', 'date'), ('comunidad', 'string'), ('provincia', 'string'), ('nombre', 'string'), ('altitud', 'int'), ('latitud', 'string'), ('longitud', 'string'), ('tmed', 'float'), ('prec', 'float'), ('tmin', 'float'), ('tmax', 'float'), ('racha', 'float')]\n"
     ]
    }
   ],
   "source": [
    "df_after_ETL2 = ETL2(df_after_ETL1)\n",
    "df_after_ETL2.show(5)\n",
    "print('Dimension de la matriz: ', df_after_ETL2.count(), 'x', len(df_after_ETL2.columns))\n",
    "print(df_after_ETL2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo valores NaN que tenemos sobre el conjunto de datos despues del preprocesamiento de datos. Es importante tener esto en cuenta pues si queremos trabajar con alguna de las variables que contienen estos valores, hay que procesar los dados de nuevo. \n",
    "\n",
    "Por ejemplo, para estudiar las precipitaciones a lo largo de los años en España, debemos eliminar las filas que contienen esos valores. No lo hacemos ahora pues no queremos que afecte al histórico de temperaturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+------+-------+-------+--------+----+------+----+----+-------+\n",
      "|fecha|comunidad|provincia|nombre|altitud|latitud|longitud|tmed|  prec|tmin|tmax|  racha|\n",
      "+-----+---------+---------+------+-------+-------+--------+----+------+----+----+-------+\n",
      "|    0|        0|        0|     0|      0|      0|       0|   0|219836|   0|   0|1306240|\n",
      "+-----+---------+---------+------+-------+-------+--------+----+------+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "null_counts = df_after_ETL2.agg(*[F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df_after_ETL2.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop() # Este comando detendrá la sesión de Spark actual y liberará los recursos asociados con ella\n",
    "# del spark # eliminar completamente la variable spark de tu entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: green;\">SQLite</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegimos SQLite para crear la base de datos dada la versatilidad y sencillez de la misma. Para esto, primero debemos pasar la estructura de datos de apache a dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SQlite = df_after_ETL2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos una funcion que contiene las consultas necesarias para obtener los dos dataframe que necesitamos para nuestro dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_datos_historicos(conn):\n",
    "    query = \"\"\"SELECT fecha, comunidad, provincia, tmed, prec, tmin, tmax FROM my_table\"\"\"\n",
    "\n",
    "    df_historico = pd.read_sql_query(query, conn)\n",
    "    return df_historico\n",
    "\n",
    "def consulta_datos_TOP5(conn):\n",
    "    query = \"\"\"SELECT DISTINCT nombre, provincia, altitud, latitud, longitud, tmax, tmin, racha FROM my_table ORDER BY altitud DESC\"\"\"\n",
    "\n",
    "    df_aux = pd.read_sql_query(query, conn)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una conexión a la base de datos SQLite, guardamos nuestro df_SQlite en la misma y posteriormente se realizan las consultas definidad con anterioridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('my_database.db')\n",
    "df_SQlite.to_sql('my_table', conn, if_exists='replace', index=False)\n",
    "df_historico = consulta_datos_historicos(conn)\n",
    "df_aux = consulta_datos_TOP5(conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color: green;\">Visualización</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los datos para poder representar adecuadamente en nuestro dashboard los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "top_5_altura = df_aux.head(5).copy()\n",
    "bottom_5_altura = df_aux.sort_values(by='altitud').head(5).copy()\n",
    "top_5_tmax = df_aux.sort_values(by='tmax', ascending=False).head(5).copy()\n",
    "top_5_tmin = df_aux.sort_values(by='tmin', ascending=True).head(5).copy()\n",
    "top_5_racha = df_aux.sort_values(by='racha', ascending=False).head(5).copy()\n",
    "\n",
    "top_5_altura.loc[:,'clase'] = 'Top 5 alturas más altas'\n",
    "bottom_5_altura.loc[:,'clase'] = 'Top 5 alturas más bajas'\n",
    "top_5_tmax.loc[:,'clase'] = 'Top 5 temperaturas máximas'\n",
    "top_5_tmin.loc[:,'clase'] = 'Top 5 temperaturas mínimas'\n",
    "top_5_racha.loc[:,'clase'] = 'Top 5 rachas de viento'\n",
    "\n",
    "df_map = pd.concat([top_5_altura, bottom_5_altura, top_5_tmax, top_5_tmin, top_5_racha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna de latitud y longitud está dada en grados por lo que necesitamos pasar los datos a formato decimal si posteriormente se quieren representar en un mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def dms_to_decimal(dms):\n",
    "    degrees, minutes, seconds, direction = re.match('(\\d{2})(\\d{2})(\\d{2})(\\w)', dms).groups()\n",
    "    decimal = int(degrees) + int(minutes)/60 + int(seconds)/(60*60)\n",
    "    if direction in ['S','W']:\n",
    "        decimal *= -1\n",
    "    return decimal\n",
    "\n",
    "df_map['latitud'] = df_map['latitud'].apply(dms_to_decimal)\n",
    "df_map['longitud'] = df_map['longitud'].apply(dms_to_decimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los datos para estudiar la temperatura y la precipitacion a lo largo de los años, a nivel provincial y autonómico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df = df_historico\n",
    "final_df['fecha'] = pd.to_datetime(final_df['fecha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se presenta un pequeño dashboard de la evolución de las temperaturas y precipitaciones anuales con un selector para la provincia, la comunidad autónoma y el tipo de temperatura; así como una línea de tendencia (regresion lineal) que aporta información a cerca del aumento de temperaturas en el intervalo de tiempo preseleccionado y la tentencia que sigue en el mismo. Además, se representa en un mapa, la información relativa al TOP 5 histórico de temperatura máxima, minima, altura máxima y mínima de la estación y la racha máxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    },
    {
     "data": {
      "application/javascript": "window.open('http://127.0.0.1:8050/')",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Suponiendo que final_df está previamente definido\n",
    "unique_years = pd.to_datetime(final_df['fecha']).dt.year.unique()\n",
    "comunidades = final_df['comunidad'].unique()\n",
    "provincias = final_df['provincia'].unique()\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "\n",
    "    html.Div([\n",
    "            dcc.RadioItems(\n",
    "                id='filter-type',\n",
    "                options=[{'label': 'Comunidad', 'value': 'comunidad'}, {'label': 'Provincia', 'value': 'provincia'}],\n",
    "                value='provincia'\n",
    "            ),\n",
    "            dcc.RadioItems(\n",
    "                id='temp-or-prec',\n",
    "                options=[{'label': 'Temperatura', 'value': 'temperatura'}, {'label': 'Precipitación', 'value': 'precipitacion'}],\n",
    "                value='temperatura'\n",
    "            ),\n",
    "    ], style={'display': 'flex', 'flex-direction': 'row'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='comunidad-dropdown',\n",
    "            options=[{'label': com, 'value': com} for com in final_df['comunidad'].unique()],\n",
    "            value='Castilla y León',\n",
    "            clearable=False\n",
    "        ),\n",
    "    ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='provincia-dropdown',\n",
    "            options=[{'label': prov, 'value': prov} for prov in final_df['provincia'].unique()],\n",
    "            value='LEON',\n",
    "            clearable=False\n",
    "        ),\n",
    "    ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='temp-dropdown',\n",
    "            options=[{'label': 'Minimas', 'value': 'Minimas'}, {'label': 'Maximas', 'value': 'Maximas'}, {'label': 'Medias', 'value': 'Medias'}],\n",
    "            value='Medias',\n",
    "            clearable=False\n",
    "        ),\n",
    "    ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='data-frequency-dropdown',\n",
    "            options=[{'label': 'Diario', 'value': 'diarias'}, {'label': 'Mensual', 'value': 'mensuales'}, {'label': 'Anual', 'value': 'anuales'}],\n",
    "            value='diarias',\n",
    "            clearable=False\n",
    "        ),\n",
    "    ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.RangeSlider(\n",
    "            id='year-slider',\n",
    "            min=unique_years.min(),\n",
    "            max=unique_years.max(),\n",
    "            value=[unique_years.min(), unique_years.max()],\n",
    "            marks={str(year): str(year) for year in unique_years}\n",
    "        )\n",
    "    ], style={'width': '100%', 'margin': 'auto'}),\n",
    "\n",
    "    dcc.Graph(id='temperature-graph'),\n",
    "    dcc.Graph(id='geo-scatter-graph')  \n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('temperature-graph', 'figure'),\n",
    "    [Input('temp-dropdown', 'value'),\n",
    "    Input('provincia-dropdown', 'value'),\n",
    "    Input('comunidad-dropdown', 'value'),\n",
    "    Input('year-slider', 'value'),\n",
    "    Input('filter-type', 'value'),\n",
    "    Input('temp-or-prec', 'value'),\n",
    "    Input('data-frequency-dropdown', 'value')],\n",
    ")\n",
    "def update_graph(selected_temp, selected_provincia, selected_comunidad, selected_years, filter_type, selected_temp_or_prec, data_frequency):\n",
    "    if filter_type == 'comunidad':\n",
    "        filtered_df = final_df[(final_df['comunidad'] == selected_comunidad) & (final_df['fecha'].dt.year.between(selected_years[0], selected_years[1]))].copy()\n",
    "    else:\n",
    "        filtered_df = final_df[(final_df['provincia'] == selected_provincia) & (final_df['fecha'].dt.year.between(selected_years[0], selected_years[1]))].copy()\n",
    "\n",
    "    if selected_temp_or_prec == 'temperatura':\n",
    "        if selected_temp == 'Minimas':\n",
    "            column = 'tmin'\n",
    "        elif selected_temp == 'Maximas':\n",
    "            column = 'tmax'\n",
    "        elif selected_temp == 'Medias':\n",
    "            column = 'tmed'\n",
    "        y_title = 'Temperatura (ºC)'\n",
    "    elif selected_temp_or_prec == 'precipitacion':\n",
    "        column = 'prec'\n",
    "        y_title = 'Precipitación (mm)'\n",
    "        filtered_df = filtered_df.dropna(subset=['prec'])\n",
    "    else:\n",
    "        raise ValueError('selected_temp_or_prec debe ser \"temperatura\" o \"precipitacion\"')\n",
    "\n",
    "    filtered_df['fecha'] = pd.to_datetime(filtered_df['fecha'])\n",
    "    filtered_df['year'] = filtered_df['fecha'].dt.year\n",
    "    filtered_df['month'] = filtered_df['fecha'].dt.month\n",
    "\n",
    "    if data_frequency == 'diarias':\n",
    "        data = filtered_df.groupby('fecha')[column].mean().reset_index()\n",
    "        data['year_month'] = data['fecha'].dt.strftime('%Y-%m')\n",
    "        x_valor = 'fecha'\n",
    "    elif data_frequency == 'mensuales':\n",
    "        data = filtered_df.groupby(['year', 'month'])[column].mean().reset_index()\n",
    "        data['year_month'] = data['year'].astype(str) + '-' + data['month'].astype(str).str.zfill(2)\n",
    "        x_valor = 'year_month'\n",
    "    elif data_frequency == 'anuales':\n",
    "        data = filtered_df.groupby(['year'])[column].mean().reset_index()\n",
    "        data['year_month'] = data['year'].astype(str)\n",
    "        x_valor = 'year_month'\n",
    "    else:\n",
    "        raise ValueError('data_frequency debe ser \"diarias\", \"mensuales\" o \"anuales\"')\n",
    "\n",
    "    fig = px.line(data, x=x_valor, y=column, title=f'Promedio {selected_temp_or_prec} {selected_temp.lower()} {data_frequency.lower()} de {selected_provincia if filter_type == \"provincia\" else selected_comunidad}', color_discrete_sequence=['blue'])\n",
    "\n",
    "    X = data.index.values.reshape(-1, 1)\n",
    "    y = data[column]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    data['y_pred'] = y_pred\n",
    "    diff = data.y_pred.iloc[-1] - data.y_pred.iloc[0]\n",
    "    if selected_temp_or_prec == 'temperatura':\n",
    "        fig.add_trace(go.Scatter(x=data['year_month'], y=data['y_pred'], mode='lines', name=f'Trendline ({diff:+.2f}ºC)', line=dict(color='red', width=2)))\n",
    "    else:\n",
    "        fig.add_trace(go.Scatter(x=data['year_month'], y=data['y_pred'], mode='lines', name=f'Trendline ({diff:+.2f}mm)', line=dict(color='red', width=2)))\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='rgba(1, 1, 1, 0.05)',\n",
    "        xaxis_title='Año',\n",
    "        yaxis_title=y_title,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('geo-scatter-graph', 'figure'),\n",
    "    [Input('filter-type', 'value')]\n",
    ")\n",
    "def update_geo_graph(filter_type):\n",
    "\n",
    "    fig = px.scatter_geo(\n",
    "        df_map,\n",
    "        lat='latitud',\n",
    "        lon='longitud',\n",
    "        color='clase', \n",
    "        hover_data=['altitud', 'tmax', 'tmin', 'racha'],\n",
    "        hover_name='nombre',\n",
    "        projection='natural earth',\n",
    "        title='TOP 5 histórico de estaciones meteorológicas en función de altura, temperaturas y rachas de viento'\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        geo=dict(\n",
    "            showland=True,\n",
    "            landcolor=\"green\",\n",
    "            showcoastlines=True,\n",
    "            coastlinecolor=\"black\",\n",
    "            showocean=True,\n",
    "            oceancolor=\"lightblue\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('temp-dropdown', 'options'),\n",
    "    [Input('temp-or-prec', 'value')]\n",
    ")\n",
    "def update_temp_dropdown(selected_temp_or_prec):\n",
    "    if selected_temp_or_prec == 'temperatura':\n",
    "                return [{'label': 'Minimas', 'value': 'Minimas'}, {'label': 'Maximas', 'value': 'Maximas'}, {'label': 'Medias', 'value': 'Medias'}]\n",
    "    elif selected_temp_or_prec == 'precipitacion':\n",
    "        return [{'label': 'Precipitación', 'value': 'precipitacion'}]\n",
    "\n",
    "@app.callback(\n",
    "    Output('provincia-dropdown', 'options'),\n",
    "    [Input('filter-type', 'value')]\n",
    ")\n",
    "def update_provincia_dropdown(filter_type):\n",
    "    if filter_type == 'provincia':\n",
    "        return [{'label': provincia, 'value': provincia} for provincia in provincias]\n",
    "    elif filter_type == 'comunidad':\n",
    "        return []\n",
    "\n",
    "@app.callback(\n",
    "    Output('comunidad-dropdown', 'options'),\n",
    "    [Input('filter-type', 'value')]\n",
    ")\n",
    "def update_comunidad_dropdown(filter_type):\n",
    "    if filter_type == 'comunidad':\n",
    "        return [{'label': comunidad, 'value': comunidad} for comunidad in comunidades]\n",
    "    elif filter_type == 'provincia':\n",
    "        return []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run_server(debug=True)\n",
    "    app.run(jupyter_mode=\"tab\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
